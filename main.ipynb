{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a63945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Use NLTK and the specific corpus\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44aa1dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\anu\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\anu\\anaconda3\\lib\\site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\anu\\anaconda3\\lib\\site-packages (from flask) (2.11.3)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\anu\\anaconda3\\lib\\site-packages (from flask) (8.0.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\anu\\anaconda3\\lib\\site-packages (from flask) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anu\\anaconda3\\lib\\site-packages (from click>=5.1->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\anu\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347abc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\anu\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0355b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBC Dataset Load and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc226fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Values: title          0\n",
      "pubDate        0\n",
      "link           0\n",
      "description    0\n",
      "publisher      0\n",
      "dtype: int64\n",
      "Column 'title' does not have any NaN values.\n",
      "Column 'pubDate' does not have any NaN values.\n",
      "Column 'link' does not have any NaN values.\n",
      "Column 'description' does not have any NaN values.\n",
      "Column 'publisher' does not have any NaN values.\n",
      "Number of Duplicate Rows: 0\n",
      "Sum of rows not containing URLs: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>does_not_contain_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ukraine Angry Zelensky vows to punish Russian ...</td>\n",
       "      <td>Mon, 07 Mar 2022 08:01:56 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-606380...</td>\n",
       "      <td>The Ukrainian president says the country will ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>War in Ukraine Taking cover in a town under at...</td>\n",
       "      <td>Sun, 06 Mar 2022 22:49:58 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-europe-606418...</td>\n",
       "      <td>Jeremy Bowen was on the frontline in Irpin as ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ukraine war catastrophic for global food</td>\n",
       "      <td>Mon, 07 Mar 2022 00:14:42 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-60623941?a...</td>\n",
       "      <td>One of the worlds biggest fertiliser firms say...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manchester Arena bombing Saffie Roussoss paren...</td>\n",
       "      <td>Mon, 07 Mar 2022 00:05:40 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/uk-60579079?at_medi...</td>\n",
       "      <td>The parents of the Manchester Arena bombings y...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ukraine conflict Oil price soars to highest le...</td>\n",
       "      <td>Mon, 07 Mar 2022 08:15:53 GMT</td>\n",
       "      <td>https://www.bbc.co.uk/news/business-60642786?a...</td>\n",
       "      <td>Consumers are feeling the impact of higher ene...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ukraine Angry Zelensky vows to punish Russian ...   \n",
       "1  War in Ukraine Taking cover in a town under at...   \n",
       "2           Ukraine war catastrophic for global food   \n",
       "3  Manchester Arena bombing Saffie Roussoss paren...   \n",
       "4  Ukraine conflict Oil price soars to highest le...   \n",
       "\n",
       "                         pubDate  \\\n",
       "0  Mon, 07 Mar 2022 08:01:56 GMT   \n",
       "1  Sun, 06 Mar 2022 22:49:58 GMT   \n",
       "2  Mon, 07 Mar 2022 00:14:42 GMT   \n",
       "3  Mon, 07 Mar 2022 00:05:40 GMT   \n",
       "4  Mon, 07 Mar 2022 08:15:53 GMT   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.bbc.co.uk/news/world-europe-606380...   \n",
       "1  https://www.bbc.co.uk/news/world-europe-606418...   \n",
       "2  https://www.bbc.co.uk/news/business-60623941?a...   \n",
       "3  https://www.bbc.co.uk/news/uk-60579079?at_medi...   \n",
       "4  https://www.bbc.co.uk/news/business-60642786?a...   \n",
       "\n",
       "                                         description publisher  \\\n",
       "0  The Ukrainian president says the country will ...       BBC   \n",
       "1  Jeremy Bowen was on the frontline in Irpin as ...       BBC   \n",
       "2  One of the worlds biggest fertiliser firms say...       BBC   \n",
       "3  The parents of the Manchester Arena bombings y...       BBC   \n",
       "4  Consumers are feeling the impact of higher ene...       BBC   \n",
       "\n",
       "   does_not_contain_url  \n",
       "0                 False  \n",
       "1                 False  \n",
       "2                 False  \n",
       "3                 False  \n",
       "4                 False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df_bbc = pd.read_csv('bbc_news.csv')  \n",
    "\n",
    "# Specify the name of the column want to remove\n",
    "column_name_to_remove = 'guid'  \n",
    "\n",
    "# Use the `drop()` method to remove the specified column\n",
    "df_bbc = df_bbc.drop(column_name_to_remove, axis=1)\n",
    "\n",
    "df_bbc['publisher'] = 'BBC'\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df_bbc.isnull().sum()\n",
    "\n",
    "print(f\"Number of Missing Values: {missing_values}\")\n",
    "\n",
    "# Check for NaN values in each column\n",
    "for column in df_bbc.columns:\n",
    "    if df_bbc[column].isna().any():\n",
    "        print(f\"Column '{column}' has NaN values.\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' does not have any NaN values.\")\n",
    "\n",
    "# Check for duplicate rows based on all columns\n",
    "duplicates = df_bbc.duplicated()\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "df_bbc.drop_duplicates(inplace=True)\n",
    "\n",
    "# To display the duplicate rows (optional)\n",
    "#duplicate_rows = df[duplicates]\n",
    "#print(\"Duplicate Rows:\")\n",
    "#print(duplicate_rows)\n",
    "\n",
    "# To count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of Duplicate Rows: {num_duplicates}\")\n",
    "\n",
    "# Specify the column want to check for URLs\n",
    "column_name = 'link' \n",
    "\n",
    "# Define a regular expression pattern to match URLs\n",
    "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "# Use the inverse of str.contains() to check for rows that do not contain URLs\n",
    "df_bbc['does_not_contain_url'] = ~df_bbc[column_name].str.contains(url_pattern, case=False, flags=re.IGNORECASE)\n",
    "\n",
    "# Calculate the sum of rows that do not contain URLs\n",
    "sum_not_containing_url = df_bbc['does_not_contain_url'].sum()\n",
    "\n",
    "# Display the sum\n",
    "print(f\"Sum of rows not containing URLs: {sum_not_containing_url}\")\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Preprocess the text column\n",
    "df_bbc['title'] = df_bbc['title'].apply(remove_special_characters)\n",
    "df_bbc['description'] = df_bbc['description'].apply(remove_special_characters)\n",
    "\n",
    "\n",
    "# Specify the column for calculate the sum of unique values\n",
    "column_name = 'link'  \n",
    "column_name_two = 'description'  \n",
    "\n",
    "# Calculate the sum of unique values in the specified column\n",
    "unique_sum = df_bbc[column_name].nunique()\n",
    "unique_sum_two = df_bbc[column_name_two].nunique()\n",
    "\n",
    "# Display the sum of unique values\n",
    "#print(f\"Sum of unique values in '{column_name}': {unique_sum}\")\n",
    "#print(f\"Sum of unique values in '{column_name_two}': {unique_sum_two}\")\n",
    "\n",
    "# Specify the column want to check for identical data\n",
    "column_name = 'link'  \n",
    "\n",
    "# Use the `duplicated()` method to create a Boolean Series indicating duplicate values in the column\n",
    "duplicates = df_bbc[column_name].duplicated(keep=False)\n",
    "\n",
    "# Filter the DataFrame to include only rows with identical data in the specified column\n",
    "identical_data_rows = df_bbc[duplicates].sum()\n",
    "\n",
    "# Display the rows with identical data in the specified column\n",
    "#print(\"Rows with Identical Data in '{}' Column:\".format(column_name))\n",
    "#print(identical_data_rows)\n",
    "\n",
    "df_bbc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN News Dataset Load and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c66b80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Values: pubDate        0\n",
      "link           0\n",
      "title          0\n",
      "description    0\n",
      "publisher      0\n",
      "dtype: int64\n",
      "Column 'pubDate' does not have any NaN values.\n",
      "Column 'link' does not have any NaN values.\n",
      "Column 'title' does not have any NaN values.\n",
      "Column 'description' does not have any NaN values.\n",
      "Column 'publisher' does not have any NaN values.\n",
      "Number of Duplicate Rows: 0\n",
      "Sum of rows not containing URLs: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubDate</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>does_not_contain_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-15 02:46:59</td>\n",
       "      <td>https://www.cnn.com/2021/07/14/world/tusimple-...</td>\n",
       "      <td>Theres a shortage of truckers but TuSimple thi...</td>\n",
       "      <td>The ecommerce boom has exacerbated a global tr...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-12 07:52:09</td>\n",
       "      <td>https://www.cnn.com/2021/05/12/world/ironhand-...</td>\n",
       "      <td>Bioservos robotic Ironhand could protect facto...</td>\n",
       "      <td>Working in a factory can mean doing the same t...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-16 02:51:30</td>\n",
       "      <td>https://www.cnn.com/2021/06/15/asia/swarm-robo...</td>\n",
       "      <td>This swarm of robots gets smarter the more it ...</td>\n",
       "      <td>In a Hong Kong warehouse a swarm of autonomous...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-18 14:37:21</td>\n",
       "      <td>https://www.cnn.com/2022/03/18/success/pandemi...</td>\n",
       "      <td>Two years later remote work has changed millio...</td>\n",
       "      <td>Heres a look at how the pandemic reshaped peop...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-19 11:41:08</td>\n",
       "      <td>https://www.cnn.com/2022/03/19/investing/march...</td>\n",
       "      <td>Why March is so volatile for stocks  CNN</td>\n",
       "      <td>March Madness isnt just for college basketball...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pubDate                                               link  \\\n",
       "0  2021-07-15 02:46:59  https://www.cnn.com/2021/07/14/world/tusimple-...   \n",
       "1  2021-05-12 07:52:09  https://www.cnn.com/2021/05/12/world/ironhand-...   \n",
       "2  2021-06-16 02:51:30  https://www.cnn.com/2021/06/15/asia/swarm-robo...   \n",
       "3  2022-03-18 14:37:21  https://www.cnn.com/2022/03/18/success/pandemi...   \n",
       "4  2022-03-19 11:41:08  https://www.cnn.com/2022/03/19/investing/march...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Theres a shortage of truckers but TuSimple thi...   \n",
       "1  Bioservos robotic Ironhand could protect facto...   \n",
       "2  This swarm of robots gets smarter the more it ...   \n",
       "3  Two years later remote work has changed millio...   \n",
       "4           Why March is so volatile for stocks  CNN   \n",
       "\n",
       "                                         description publisher  \\\n",
       "0  The ecommerce boom has exacerbated a global tr...       CNN   \n",
       "1  Working in a factory can mean doing the same t...       CNN   \n",
       "2  In a Hong Kong warehouse a swarm of autonomous...       CNN   \n",
       "3  Heres a look at how the pandemic reshaped peop...       CNN   \n",
       "4  March Madness isnt just for college basketball...       CNN   \n",
       "\n",
       "   does_not_contain_url  \n",
       "0                 False  \n",
       "1                 False  \n",
       "2                 False  \n",
       "3                 False  \n",
       "4                 False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df_cnn = pd.read_csv('CNN_news.csv')  \n",
    "\n",
    "# Specify the names of the columns want to remove\n",
    "columns_to_remove = ['Author', 'Index','Category', 'Section','Keywords','Second headline','Article text']  \n",
    "\n",
    "# Use the `drop()` method to remove the specified columns\n",
    "df_cnn = df_cnn.drop(columns=columns_to_remove)\n",
    "\n",
    "# Create a dictionary to map the old column names to new column names\n",
    "column_mapping = {\n",
    "    'Headline': 'title',\n",
    "    'Date published': 'pubDate',\n",
    "    'Url': 'link',\n",
    "    'Description': 'description',\n",
    "}\n",
    "\n",
    "# Rename the columns using the rename() function\n",
    "df_cnn.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add a new column with the default value\n",
    "df_cnn['publisher'] = 'CNN'\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df_cnn.isnull().sum()\n",
    "\n",
    "print(f\"Number of Missing Values: {missing_values}\")\n",
    "\n",
    "# Check for NaN values in each column\n",
    "for column in df_cnn.columns:\n",
    "    if df_cnn[column].isna().any():\n",
    "        print(f\"Column '{column}' has NaN values.\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' does not have any NaN values.\")\n",
    "        \n",
    "# Check for duplicate rows based on all columns\n",
    "duplicates = df_cnn.duplicated()\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "df_cnn.drop_duplicates(inplace=True)\n",
    "\n",
    "# To display the duplicate rows (optional)\n",
    "#duplicate_rows = df[duplicates]\n",
    "#print(\"Duplicate Rows:\")\n",
    "#print(duplicate_rows)\n",
    "\n",
    "# To count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of Duplicate Rows: {num_duplicates}\")\n",
    "\n",
    "# Specify the column want to check for URLs\n",
    "column_name = 'link'  \n",
    "\n",
    "# Define a regular expression pattern to match URLs\n",
    "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "# Use the inverse of str.contains() to check for rows that do not contain URLs\n",
    "df_cnn['does_not_contain_url'] = ~df_cnn[column_name].str.contains(url_pattern, case=False, flags=re.IGNORECASE)\n",
    "\n",
    "# Calculate the sum of rows that do not contain URLs\n",
    "sum_not_containing_url = df_cnn['does_not_contain_url'].sum()\n",
    "\n",
    "# Display the sum\n",
    "print(f\"Sum of rows not containing URLs: {sum_not_containing_url}\")\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Preprocess the text column\n",
    "df_cnn['title'] = df_cnn['title'].apply(remove_special_characters)\n",
    "df_cnn['description'] = df_cnn['description'].apply(remove_special_characters)\n",
    "\n",
    "# Specify the column for which want to calculate the sum of unique values\n",
    "column_name = 'link' \n",
    "column_name_two = 'title'\n",
    "\n",
    "# Calculate the sum of unique values in the specified column\n",
    "unique_sum = df_cnn[column_name].nunique()\n",
    "unique_sum_two = df_cnn[column_name_two].nunique()\n",
    "\n",
    "# Display the sum of unique values\n",
    "#print(f\"Sum of unique values in '{column_name}': {unique_sum}\")\n",
    "#print(f\"Sum of unique values in '{column_name_two}': {unique_sum_two}\")\n",
    "\n",
    "# Specify the column want to check for identical data\n",
    "column_name = 'title'\n",
    "\n",
    "# Use the `duplicated()` method to create a Boolean Series indicating duplicate values in the column\n",
    "duplicates = df_cnn[column_name].duplicated(keep=False)\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "df_cnn.drop_duplicates(inplace=True)\n",
    "\n",
    "# Filter the DataFrame to include only rows with identical data in the specified column\n",
    "identical_data_rows = df_cnn[duplicates].sum()\n",
    "\n",
    "# Display the rows with identical data in the specified column\n",
    "#print(\"Rows with Identical Data in '{}' Column:\".format(column_name))\n",
    "#print(identical_data_rows)\n",
    "\n",
    "df_cnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNBC News Dataset Load and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa7f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing Values: title           0\n",
      "link            0\n",
      "pubDate         0\n",
      "publisher       0\n",
      "description    32\n",
      "dtype: int64\n",
      "Column 'title' does not have any NaN values.\n",
      "Column 'link' does not have any NaN values.\n",
      "Column 'pubDate' does not have any NaN values.\n",
      "Column 'publisher' does not have any NaN values.\n",
      "Column 'description' has NaN values.\n",
      "Number of Duplicate Rows: 0\n",
      "Sum of rows not containing URLs: 0\n",
      "Sum of unique values in 'link': 625\n",
      "Sum of unique values in 'title': 625\n",
      "Rows with Identical Data in 'link' Column:\n",
      "title                   0.0\n",
      "link                    0.0\n",
      "pubDate                 0.0\n",
      "publisher               0.0\n",
      "description             0.0\n",
      "does_not_contain_url    0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>publisher</th>\n",
       "      <th>description</th>\n",
       "      <th>does_not_contain_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Santolis Wednesday market notes Could Septembe...</td>\n",
       "      <td>https://www.cnbc.com/2021/09/29/santolis-wedne...</td>\n",
       "      <td>2021-09-29T17:09:39+0000</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>This is the daily notebook of Mike Santoli CNB...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My take on the early Brexit winners and losers</td>\n",
       "      <td>https://www.cnbc.com/2016/06/24/ian-bremmers-t...</td>\n",
       "      <td>2016-06-24T13:50:48-0400</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>My take on the early Brexit winners and losers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europe039s recovery depends on Renzi039s Italy</td>\n",
       "      <td>https://www.cnbc.com/2014/03/25/europes-recove...</td>\n",
       "      <td>2014-03-25T13:29:45-0400</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>Europe039s recovery depends on Renzi039s Italy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US Moves Closer to Becoming A Major Shareholde...</td>\n",
       "      <td>https://www.cnbc.com/2009/04/22/us-moves-close...</td>\n",
       "      <td>2009-04-22T19:49:03+0000</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>The US government is increasingly likely to co...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump Mission accomplished on perfectly execut...</td>\n",
       "      <td>https://www.cnbc.com/2018/04/14/trump-mission-...</td>\n",
       "      <td>2018-04-14T14:59:04+0000</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>President Donald Trump hailed the USled interv...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Santolis Wednesday market notes Could Septembe...   \n",
       "1     My take on the early Brexit winners and losers   \n",
       "2     Europe039s recovery depends on Renzi039s Italy   \n",
       "3  US Moves Closer to Becoming A Major Shareholde...   \n",
       "4  Trump Mission accomplished on perfectly execut...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.cnbc.com/2021/09/29/santolis-wedne...   \n",
       "1  https://www.cnbc.com/2016/06/24/ian-bremmers-t...   \n",
       "2  https://www.cnbc.com/2014/03/25/europes-recove...   \n",
       "3  https://www.cnbc.com/2009/04/22/us-moves-close...   \n",
       "4  https://www.cnbc.com/2018/04/14/trump-mission-...   \n",
       "\n",
       "                    pubDate publisher  \\\n",
       "0  2021-09-29T17:09:39+0000      CNBC   \n",
       "1  2016-06-24T13:50:48-0400      CNBC   \n",
       "2  2014-03-25T13:29:45-0400      CNBC   \n",
       "3  2009-04-22T19:49:03+0000      CNBC   \n",
       "4  2018-04-14T14:59:04+0000      CNBC   \n",
       "\n",
       "                                         description  does_not_contain_url  \n",
       "0  This is the daily notebook of Mike Santoli CNB...                 False  \n",
       "1     My take on the early Brexit winners and losers                 False  \n",
       "2     Europe039s recovery depends on Renzi039s Italy                 False  \n",
       "3  The US government is increasingly likely to co...                 False  \n",
       "4  President Donald Trump hailed the USled interv...                 False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df_cnbc = pd.read_csv('cnbc_news.csv') \n",
    "\n",
    "\n",
    "# Specify the names of the columns want to remove\n",
    "columns_to_remove = ['author', 'short_description','header_image', 'keywords','raw_description','scraped_at']\n",
    "\n",
    "# Use the `drop()` method to remove the specified columns\n",
    "df_cnbc = df_cnbc.drop(columns=columns_to_remove)\n",
    "\n",
    "# Create a dictionary to map the old column names to new column names\n",
    "column_mapping = {\n",
    "    'published_at': 'pubDate',\n",
    "    'url': 'link',\n",
    "}\n",
    "\n",
    "# Rename the columns using the rename() function\n",
    "df_cnbc.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df_cnbc.isnull().sum()\n",
    "\n",
    "print(f\"Number of Missing Values: {missing_values}\")\n",
    "\n",
    "# Check for NaN values in each column\n",
    "for column in df_cnbc.columns:\n",
    "    if df_cnbc[column].isna().any():\n",
    "        print(f\"Column '{column}' has NaN values.\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' does not have any NaN values.\")\n",
    "\n",
    "# Replace NaN values with the corresponding title value\n",
    "df_cnbc['description'] = df_cnbc['description'].fillna(df_cnbc['title'])\n",
    "\n",
    "# Check for duplicate rows based on all columns\n",
    "duplicates = df_cnbc.duplicated()\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "df_cnbc.drop_duplicates(inplace=True)\n",
    "\n",
    "# To display the duplicate rows (optional)\n",
    "#duplicate_rows = df[duplicates]\n",
    "#print(\"Duplicate Rows:\")\n",
    "#print(duplicate_rows)\n",
    "\n",
    "# To count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of Duplicate Rows: {num_duplicates}\")\n",
    "\n",
    "# Specify the column want to check for URLs\n",
    "column_name = 'link'\n",
    "\n",
    "# Define a regular expression pattern to match URLs\n",
    "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "# Use the inverse of str.contains() to check for rows that do not contain URLs\n",
    "df_cnbc['does_not_contain_url'] = ~df_cnbc[column_name].str.contains(url_pattern, case=False, flags=re.IGNORECASE)\n",
    "\n",
    "# Calculate the sum of rows that do not contain URLs\n",
    "sum_not_containing_url = df_cnbc['does_not_contain_url'].sum()\n",
    "\n",
    "# Display the sum\n",
    "print(f\"Sum of rows not containing URLs: {sum_not_containing_url}\")\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string-like object\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "# Preprocess the text column\n",
    "df_cnbc['title'] = df_cnbc['title'].apply(remove_special_characters)\n",
    "df_cnbc['description'] = df_cnbc['description'].apply(remove_special_characters)\n",
    "\n",
    "# Specify the column for which want to calculate the sum of unique values\n",
    "column_name = 'link' \n",
    "column_name_two = 'title'\n",
    "\n",
    "# Calculate the sum of unique values in the specified column\n",
    "unique_sum = df_cnbc[column_name].nunique()\n",
    "unique_sum_two = df_cnbc[column_name_two].nunique()\n",
    "\n",
    "# Display the sum of unique values\n",
    "print(f\"Sum of unique values in '{column_name}': {unique_sum}\")\n",
    "print(f\"Sum of unique values in '{column_name_two}': {unique_sum_two}\")\n",
    "\n",
    "# Specify the column want to check for identical data\n",
    "column_name = 'link'\n",
    "\n",
    "# Use the `duplicated()` method to create a Boolean Series indicating duplicate values in the column\n",
    "duplicates = df_cnbc[column_name].duplicated(keep=False)\n",
    "\n",
    "# Filter the DataFrame to include only rows with identical data in the specified column\n",
    "identical_data_rows = df_cnbc[duplicates].sum()\n",
    "\n",
    "# Remove duplicates from the DataFrame\n",
    "df_cnbc.drop_duplicates(inplace=True)\n",
    "\n",
    "# Display the rows with identical data in the specified column\n",
    "print(\"Rows with Identical Data in '{}' Column:\".format(column_name))\n",
    "print(identical_data_rows)\n",
    "\n",
    "df_cnbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f757af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 61582\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate the three DataFrames along the rows (simple appending)\n",
    "merged_df = pd.concat([df_bbc, df_cnn, df_cnbc], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file if needed\n",
    "merged_df.to_csv('merged_file.csv', index=False)\n",
    "\n",
    "merged_df.head()\n",
    "# Get the total number of rows in the DataFrame\n",
    "total_rows = merged_df.shape[0]\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"Total Rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anu\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/Oct/2023 14:32:24] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input is plain text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Oct/2023 14:33:17] \"POST /check_news HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'c1055998@my.shu.ac.uk', 'hijn', '0', datetime.datetime(2023, 10, 10, 18, 13, 31))\n",
      "(2, '10anu336@gmail.com', \"Ukraine war: PM calls for 'step-by-step' move from Russian fuel - BBC News\", '0', datetime.datetime(2023, 10, 12, 18, 25, 18))\n",
      "(3, 'testemail@email.com', \"Covid: Fourth jab for Scotland's vulnerable, and testing wind down fears in Wales - BBC News\", '0', datetime.datetime(2023, 10, 12, 21, 7, 20))\n",
      "(4, 'anupankriyas@gg.in', \"Ukraine invasion: Volunteers 'working on autopilot' - BBC News\", '0', datetime.datetime(2023, 10, 12, 21, 23, 38))\n",
      "(5, 'ggg@we.in', \"Ukraine invasion: Volunteers 'working on autopilot' - BBC News\", '1', datetime.datetime(2023, 10, 12, 21, 41, 2))\n",
      "(6, 'testemail@email.cok', 'Ukraine conflict: Petrol at fresh record as oil and gas prices soar - BBC News', '1', datetime.datetime(2023, 10, 16, 17, 19, 55))\n",
      "(7, 'testemailone@email.com', 'Twitter is part of our war effort - Ukraine minister', '1', datetime.datetime(2023, 10, 22, 14, 29, 20))\n",
      "(8, 'checkemail@test.com', \"Ukraine invasion: Volunteers 'working on autopilot'\", '1', datetime.datetime(2023, 10, 22, 14, 33, 17))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pymysql\n",
    "\n",
    "import time\n",
    "from flask import Flask, render_template, request, g\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.before_request\n",
    "def before_request():\n",
    "    g.request_start_time = time.time()\n",
    "    g.request_time = lambda: \"%.5fs\" % (time.time() - g.request_start_time)\n",
    "\n",
    "# Function to check if the input contains a link\n",
    "def contains_link(text):\n",
    "    # Define a regular expression pattern to match URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    # Use re.search to find a match in the input\n",
    "    match = re.search(url_pattern, text)\n",
    "    \n",
    "    return match is not None\n",
    "\n",
    "# Function to parse the title from Link\n",
    "def parse_title(url):\n",
    "    html_page = urlopen(url)\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    title = soup.title.string\n",
    "    return title\n",
    "\n",
    "# Create a set of stop words \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Define a function to remove stop words from a sentence \n",
    "def remove_stop_words(sentence): \n",
    "  # Split the sentence into individual words \n",
    "  words = sentence.split() \n",
    "  \n",
    "  # Use a list comprehension to remove stop words \n",
    "  filtered_words = [word for word in words if word not in stop_words] \n",
    "  \n",
    "  # Join the filtered words back into a sentence \n",
    "  return ' '.join(filtered_words)\n",
    "\n",
    "# Function to calculate string match accuracy\n",
    "def calculate_match_accuracy(reference_string, test_string):\n",
    "    return fuzz.ratio(reference_string, test_string)\n",
    "\n",
    "# Function to perform K-Means clustering on a set of texts\n",
    "def cluster_texts(texts, num_clusters):\n",
    "    try:\n",
    "        # Create TF-IDF vectors for the texts\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Perform K-Means clustering\n",
    "        #kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        \n",
    "        # Perform Mini-Batch K-Means clustering\n",
    "        kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, batch_size=1000, max_no_improvement=10, reassignment_ratio=0.01)\n",
    "        \n",
    "        kmeans.fit(tfidf_matrix)\n",
    "\n",
    "        # Get cluster assignments for each text\n",
    "        cluster_assignments = kmeans.labels_\n",
    "\n",
    "        return cluster_assignments\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing texts: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to check user input text match accuracy against CSV file\n",
    "def check_user_input_accuracy(user_input, csv_filename, num_clusters):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Extract the text column from the CSV\n",
    "        csv_texts = df['title'].tolist()\n",
    "        \n",
    "        # Cluster both the user input and CSV texts\n",
    "        user_input_cluster_assignments = cluster_texts([user_input], num_clusters)\n",
    "        csv_cluster_assignments = cluster_texts(csv_texts, num_clusters)\n",
    "\n",
    "        # Determine the cluster of the user input text\n",
    "        user_input_cluster = user_input_cluster_assignments[0]\n",
    "\n",
    "        # Find the CSV texts that belong to the same cluster as the user input\n",
    "        matching_csv_texts = [csv_texts[i] for i, cluster in enumerate(csv_cluster_assignments) if cluster == user_input_cluster]\n",
    "\n",
    "        # Calculate accuracy by comparing the user input with each matching CSV text\n",
    "        accuracies = [calculate_match_accuracy(user_input, csv_text) for csv_text in matching_csv_texts]\n",
    "\n",
    "        # Determine the highest accuracy among the matching texts\n",
    "        max_accuracy = max(accuracies)\n",
    "\n",
    "        return max_accuracy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user input or CSV file: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Function for inserting user fake news check details in to database\n",
    "def data_insert_db(email,news_data,status):\n",
    "    # database connection\n",
    "    connection = pymysql.connect(host=\"localhost\", port=3306, user=\"root\", passwd=\"\", database=\"fake_news_data\")\n",
    "\n",
    "    cursor = connection.cursor()    \n",
    "\n",
    "    # Insert data into the table\n",
    "    insert_query = \"INSERT INTO news_articles_data (email, news_data, status) VALUES (%s, %s, %s)\"\n",
    "    cursor.execute(insert_query, (email, news_data, status))\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    connection.commit()\n",
    "\n",
    "    # Execute SQL queries\n",
    "    cursor.execute(\"SELECT * FROM news_articles_data\")\n",
    "\n",
    "    # Fetch all rows from the result set\n",
    "    result = cursor.fetchall()\n",
    "\n",
    "    # Print the results\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "    # Close the cursor and the connection when done\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "@app.route('/check_news', methods=['POST'])\n",
    "def check_news():\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['news_text']\n",
    "        email = request.form['email']\n",
    "        csv_filename = \"merged_file.csv\"\n",
    "         # Check if the user input contains a link\n",
    "        if contains_link(user_input):\n",
    "            user_input = parse_title(user_input)\n",
    "            print(\"User input contains a link.\")\n",
    "        else:\n",
    "            print(\"User input is plain text.\")\n",
    "\n",
    "        filtered_sentence = remove_stop_words(user_input) \n",
    "        num_clusters = min(1, len(user_input) + 1)   # Adjust the number of clusters as needed\n",
    "\n",
    "        accuracy = check_user_input_accuracy(user_input, csv_filename, num_clusters)\n",
    "        #status = 0\n",
    "        #if accuracy is not None:\n",
    "          #  print(f\"User Input Match Accuracy: {accuracy}%\")\n",
    "        \n",
    "        if accuracy>75:\n",
    "            status = 1\n",
    "        else:\n",
    "            status = 0\n",
    "        data_insert_db(email,user_input,status)\n",
    "    if accuracy is not None:\n",
    "       # print(f\"Match Accuracy in '{csv_filename}':\")\n",
    "        return render_template('result.html', news_text=user_input, result=accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2660f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_server():\n",
    "    func = request.environ.get('werkzeug.server.shutdown')\n",
    "    if func is None:\n",
    "        raise RuntimeError('Not running with the Werkzeug Server')\n",
    "    func()\n",
    "    \n",
    "@app.get('/shutdown')\n",
    "def shutdown():\n",
    "    shutdown_server()\n",
    "    return 'Server shutting down...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579f159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
